<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8"/>
  <title>Quality Assurance in OCR-D - OCR-D</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" href="https://avatars0.githubusercontent.com/u/26362587?s=200&amp;v=4" />
  <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous" />
  <link rel="alternate" type="application/atom+xml" title="OCR-D Blog" href="/feed.xml" />
  <link rel="stylesheet" href="/assets/bulma.css" />
  <link rel="stylesheet" href="/assets/bulma-switch.min.css" />
  <link rel="stylesheet" href="/assets/syntax-highlight.css" />
  <link rel="stylesheet" href="/assets/ocrd.css" />
  <!--katex-->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
          ],
          // • rendering keys, e.g.:
          throwOnError : true
        });
      });
    </script>
  <!--/katex-->
</head>
<body>
<script async src="https://cse.google.com/cse.js?cx=e9e3f7148e57ed66c"></script>


<script>
function ToggleSearchActive2() {
    var T = document.getElementById("button-header")
	var A = document.getElementById("google-search-header");
    T.style.display = "none";  // <-- Set it to none
	A.style.visibility = "visible";  // <-- Set it to visible
}
</script>


<nav class="navbar is-transparent is-fixed-top">

  <div class="navbar-brand">
    <a class="navbar-item" href="/">
      <img src="/assets/ocrd-logo-small.png" height="28"/>
    </a>
    <div class="navbar-burger burger" data-target="ocrd-navbar-menu">
      <span></span>
      <span></span>
      <span></span>
    </div>
  </div>

  <div id="ocrd-navbar-menu" class="navbar-menu">
    <div class="navbar-start">
      
      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" href="/en/">About</a>
        <div class="navbar-dropdown">
          

          
          

            
            <a class="navbar-item" href="/en/blog">News</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/about">About the OCR-D Project</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/phase2">Phase II: Projects</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/phase3">Phase III: Projects</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/community">Community</a>

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/publications">Publications and Presentations</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/data">Data</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/initial-tests">Pilot Study</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/user_survey">User Survey</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/contact">Contacts</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/imprint">Imprint</a>
            

          

          
        </div>
      </div>
      

      
      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" href="/en/dev">Technical Resources</a>
        <div class="navbar-dropdown">
          

          
          

            
            <a class="navbar-item" href="/en/gt-guidelines/trans">Ground Truth Guidelines</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/gt-guidelines/trans/trPage">PAGE-XML format documentation</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/dev-best-practice">OCR-D development best practices</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/spec">Specifications</a>

          

          

          
          

            
            <a class="navbar-item" href="/core">OCR-D/core API Documentation</a>

          

          
        </div>
      </div>
      

      
      
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link" href="/en/use">User Guides & Info</a>
        <div class="navbar-dropdown">
          

          
          
            
            
              
              <a class="navbar-item" href="/en/setup">Setup Guide</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/user_guide">User Guide</a>
            

          

          

          
          
            
            
              
              <a class="navbar-item" href="/en/workflows">Workflows</a>
            

          

          

          
          

            
            <a class="navbar-item" href="/en/models">Models</a>

          

          

          
          

            
            <a class="navbar-item" href="/en/spec/glossary">Glossary</a>

          

          
        </div>
      </div>
      

      
      
        <a class="navbar-item" href="/en/faq">FAQ</a>
      

      
	 <span class="navbar-item">
	 
            <a href="" title="View in German"></a>
				<div class="navbar-item has-dropdown is-hoverable" style="padding-right:10px">Search
				<div class="navbar-dropdown" style="font-size: 0.75em; padding:5px">For this feature, we implemented Google Programmable Search Engine. 
					If you use it, please note that cookies may be stored and Privacy Policy by Google LLC applies: 
					<a href="https://policies.google.com/privacy">https://policies.google.com/privacy</a> 
					<button onclick="ToggleSearchActive2()" id="button-header">Agree, show me Google Search!</button></div>
				</div>
				<div id="google-search-header" style="visibility: hidden">
					<div class="gcse-search"></div>
				</div>
			
	</span>
    </div>

    <div class="navbar-end">

      <span class="navbar-item">
        </span>


    </div> </div> </nav>
<div class="columns">
      
      <aside id="toc-sidebar-content" class="column is-one-third menu is-hidden-mobile">
        <ul class="menu-list column is-one-third">
  <li><a href="#quality-assurance-in-ocr-d">Quality Assurance in OCR-D</a>
    <ul>
      <li><a href="#rationale">Rationale</a></li>
      <li><a href="#evaluation-metrics">Evaluation metrics</a>
        <ul>
          <li><a href="#scope-of-these-definitions">Scope of these Definitions</a></li>
          <li><a href="#text-evaluation">Text Evaluation</a>
            <ul>
              <li><a href="#levenshtein-distance">Levenshtein Distance</a>
                <ul>
                  <li><a href="#general-example">General example</a></li>
                  <li><a href="#ocr-example">OCR example</a></li>
                </ul>
              </li>
              <li><a href="#cer-and-wer">CER and WER</a>
                <ul>
                  <li><a href="#characters">Characters</a>
                    <ul>
                      <li><a href="#examples">Examples</a></li>
                    </ul>
                  </li>
                  <li><a href="#character-error-rate-cer">Character Error Rate (CER)</a>
                    <ul>
                      <li><a href="#cer-granularity">CER Granularity</a></li>
                    </ul>
                  </li>
                  <li><a href="#word-error-rate-wer">Word Error Rate (WER)</a>
                    <ul>
                      <li><a href="#wer-granularity">WER Granularity</a></li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li><a href="#bag-of-words">Bag of Words</a></li>
            </ul>
          </li>
          <li><a href="#layout-evaluation">Layout Evaluation</a>
            <ul>
              <li><a href="#reading-order">Reading Order</a></li>
              <li><a href="#iou-intersection-over-union">IoU (Intersection over Union)</a></li>
            </ul>
          </li>
          <li><a href="#resource-utilization">Resource Utilization</a>
            <ul>
              <li><a href="#cpu-time">CPU Time</a></li>
              <li><a href="#wall-time">Wall Time</a></li>
              <li><a href="#io">I/O</a></li>
              <li><a href="#memory-usage">Memory Usage</a></li>
              <li><a href="#disk-usage">Disk Usage</a></li>
            </ul>
          </li>
          <li><a href="#unicode-normalization">Unicode normalization</a></li>
          <li><a href="#metrics-not-in-use-yet">Metrics Not in Use Yet</a>
            <ul>
              <li><a href="#gpu-metrics">GPU metrics</a>
                <ul>
                  <li><a href="#gpu-time">GPU time</a></li>
                  <li><a href="#gpu-avg-memory">GPU avg memory</a></li>
                </ul>
              </li>
              <li><a href="#text-evaluation-1">Text Evaluation</a>
                <ul>
                  <li><a href="#flexible-character-accuracy-measure">Flexible Character Accuracy Measure</a></li>
                </ul>
              </li>
              <li><a href="#layout-evalutation">Layout Evalutation</a>
                <ul>
                  <li><a href="#map-mean-average-precision">mAP (mean Average Precision)</a>
                    <ul>
                      <li><a href="#precision-and-recall">Precision and Recall</a></li>
                      <li><a href="#prediction-score">Prediction Score</a></li>
                      <li><a href="#thresholds">Thresholds</a></li>
                      <li><a href="#precision-recall-curve">Precision-Recall-Curve</a></li>
                      <li><a href="#average-precision">Average Precision</a></li>
                      <li><a href="#map-mean-average-precision-1">mAP (mean Average Precision)</a></li>
                    </ul>
                  </li>
                  <li><a href="#scenario-driven-performance-evaluation">Scenario-driven Performance Evaluation</a></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#evaluation-json-schema">Evaluation JSON schema</a></li>
      <li><a href="#tools">Tools</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </li>
</ul>

      </aside>
      <div id="toc-sidebar-toggle">&lt;&gt;</div>
      

      <main class="container content column is-two-thirds" aria-label="Content">
        <h1 id="quality-assurance-in-ocr-d">Quality Assurance in OCR-D</h1>

<h2 id="rationale">Rationale</h2>

<p>Estimating the quality of OCR requires workflows run on representative data,
evaluation metrics and evaluation tools that need to work together in a
well-defined manner to allow users to make informed decisions about which OCR
solution works best for their use case.</p>

<h2 id="evaluation-metrics">Evaluation metrics</h2>

<p>The evaluation of the success (accuracy) of OCR is a complex task for which multiple methods and metrics are available. It aims to capture quality in different aspects, such as the recognition of text, but also the detection of layout, for which different methods and metrics are needed.</p>

<p>Furthermore, the time and resources required for OCR processing also have to be captured. Here we describe the metrics that were selected for use in OCR-D, how exactly they are applied, and what was the motivation.</p>

<h3 id="scope-of-these-definitions">Scope of these Definitions</h3>

<p>At this stage (Q3 2022) these definitions serve as a basis of common understanding for the metrics used in the benchmarking presented in OCR-D QUIVER. Further implications for evaluation tools do not yet apply.</p>

<h3 id="text-evaluation">Text Evaluation</h3>

<p>The most important measure to assess the quality of OCR is the accuracy of the recognized text. The majority of metrics for this are based on the Levenshtein distance, an algorithm to compute the distance between two strings. In OCR, one of these strings is generally the Ground Truth text and the other the recognized text which is the result of an OCR.</p>

<h4 id="levenshtein-distance">Levenshtein Distance</h4>

<p>Levenshtein distance between two strings <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> is the number of edit operations needed to turn <code class="language-plaintext highlighter-rouge">a</code> into <code class="language-plaintext highlighter-rouge">b</code>. Edit operations depend on the specific variant of the algorithm but for OCR, relevant operations are deletion, insertion and substitution.</p>

<p>The Levenshtein distance forms the basis for the calculation of <a href="https://pad.gwdg.de/#CERWER">CER/WER</a>.</p>

<h5 id="general-example">General example</h5>

<p>The Levenshtein distance between “Monday” and “Tuesday” is 4, because 4 edit operations are necessary to turn “Monday” into “Tuesday”:</p>

<ul>
  <li><strong>M</strong>onday –&gt; <strong>T</strong>onday (substitution)</li>
  <li>T<strong>o</strong>nday –&gt; T<strong>u</strong>nday (substitution)</li>
  <li>Tu<strong>n</strong>day –&gt; Tu<strong>e</strong>day (substitution)</li>
  <li>Tueday –&gt; Tue<strong>s</strong>day (insertion)</li>
</ul>

<h5 id="ocr-example">OCR example</h5>

<p>Given a Ground truth that reads <code class="language-plaintext highlighter-rouge">ſind</code> and the recognized text <code class="language-plaintext highlighter-rouge">fmd</code>.</p>

<p>The Levenshtein distance between these texts is 4, because 4 edit operations are necessary to turn <code class="language-plaintext highlighter-rouge">fmd</code> into <code class="language-plaintext highlighter-rouge">ſind</code>:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">fmd</code> –&gt; <code class="language-plaintext highlighter-rouge">ſmd</code> (substitution)</li>
  <li><code class="language-plaintext highlighter-rouge">ſmd</code> –&gt; <code class="language-plaintext highlighter-rouge">ſimd</code> (insertion)</li>
  <li><code class="language-plaintext highlighter-rouge">ſimd</code> –&gt; <code class="language-plaintext highlighter-rouge">ſind</code> (substitution)</li>
</ul>

<h4 id="cer-and-wer">CER and WER</h4>

<h5 id="characters">Characters</h5>

<p>A text consists of a set of characters that have a certain meaning. A character is a glyph that represents a word, a letter in a word, or a symbol.</p>

<h6 id="examples">Examples</h6>

<ul>
  <li>the character <code class="language-plaintext highlighter-rouge">a</code> in the text <code class="language-plaintext highlighter-rouge">babst</code> represents the German letter <code class="language-plaintext highlighter-rouge">a</code></li>
  <li>the character <code class="language-plaintext highlighter-rouge">&amp;</code> represents the Latin abbreviation <code class="language-plaintext highlighter-rouge">etc.</code></li>
  <li>the character <code class="language-plaintext highlighter-rouge">☿</code> represents an Astronomical symbol for the planet Mercury</li>
</ul>

<h5 id="character-error-rate-cer">Character Error Rate (CER)</h5>

<p>The character error rate (CER) describes how many faulty characters the output of an OCR engine contains compaired to the Ground Truth text in relation to the text length.</p>

<p>Errors fall into one of the following three categories:</p>

<ul>
  <li><strong>deletion</strong>: a character that is present in the text has been deleted from the output.</li>
</ul>

<p>Example:
<img src="https://pad.gwdg.de/uploads/d7fa6f23-7c79-4fb2-ad94-7e98084c69d6.jpg" alt="A Fraktur sample reading &quot;Sonnenfinſterniſſe:&quot;" /></p>

<p>This reads <code class="language-plaintext highlighter-rouge">Sonnenfinſterniſſe:</code>. The output contains <code class="language-plaintext highlighter-rouge">Sonnenfinſterniſſe</code>, deleting <code class="language-plaintext highlighter-rouge">:</code>.</p>

<ul>
  <li><strong>substitution</strong>: a character is replaced by another character in the output.</li>
</ul>

<p>Example:</p>

<p><img src="https://pad.gwdg.de/uploads/b894049b-8d98-4fe7-ac31-71b2c9393a6c.jpg" alt="A Fraktur sample reading &quot;Die Finſterniſſe des 1801ſten Jahrs&quot;" /></p>

<p>This heading reads <code class="language-plaintext highlighter-rouge">Die Finſterniſſe des 1801ſten Jahrs</code>. The output contains <code class="language-plaintext highlighter-rouge">180iſten</code>, replacing <code class="language-plaintext highlighter-rouge">1</code> with <code class="language-plaintext highlighter-rouge">i</code>.</p>

<ul>
  <li><strong>insertion</strong>: a new character is introduced in the output.</li>
</ul>

<p>Example:</p>

<p><img src="https://pad.gwdg.de/uploads/e6b6432e-d79c-4568-9aef-15a026c05b39.jpg" alt="A Fraktur sample reading &quot;diese Strahlen, und&quot;" /></p>

<p>This reads <code class="language-plaintext highlighter-rouge">diese Strahlen, und</code>. The output contains <code class="language-plaintext highlighter-rouge">Strahlen ,</code>, inserting a white space before the comma.</p>

<p>CER can be calculated in several ways, depending on whether a normalized CER is used or not.</p>

<p>Given $i$ as the number of insertions, $d$ the number of deletions, $s$ the number of substitutions and $n$ the total number of characters in a text, the CER can be obtained by</p>

<p>$CER = \frac{i + s+ d}{n}$</p>

<p>If the CER value is calculated this way, it represents the percentage of characters incorrectly recognized by the OCR engine. Also, we can easily reach error rates beyond 100% when the output contains a lot of insertions.</p>

<p>The <em>normalized</em> CER tries to mitigate this effect by considering the number of correct characters, $c$:</p>

<p>$CER_n = \frac{i + s+ d}{i + s + d + c}$</p>

<p>In OCR-D’s benchmarking we calculate the <em>non-normalized</em> CER where values over 1 should be read as 100%.</p>

<h6 id="cer-granularity">CER Granularity</h6>

<p>In OCR-D we distinguish between the CER per <strong>page</strong> and the <strong>overall</strong> CER of a text. The reasoning behind this is that the material OCR-D mainly aims at (historical prints) is very heterogeneous: Some pages might have an almost simplistic layout while others can be highly complex and difficult to process. Providing only an overall CER would cloud these differences between pages.</p>

<p>At this point we only provide a CER per page; an overall CER might be calculated as a weighted aggregate at a later stage.</p>

<h5 id="word-error-rate-wer">Word Error Rate (WER)</h5>

<p>The word error rate (WER) is closely connected to the CER. While the CER focusses on differences between characters, the WER represents the percentage of words incorrectly recognized in a text.</p>

<p>CER and WER share categories of errors, and the WER is similarly calculated:</p>

<p>$WER = \frac{i_w + s_w + d_w}{n_w}$</p>

<p>where $i_w$ is the number of inserted, $s_w$ the number of substituted, $d_w$ the number of deleted and $n_w$ the total number of words.</p>

<p>More specific cases of WER consider only the “significant” words, omitting e.g. stopwords from the calculation.</p>

<h6 id="wer-granularity">WER Granularity</h6>

<p>In OCR-D we distinguish between the WER per <strong>page</strong> and the <strong>overall</strong> WER of a text. The reasoning here follows the one of CER granularity.</p>

<p>At this point we only provide a WER per page; an overall WER might be calculated at a later stage.</p>

<h4 id="bag-of-words">Bag of Words</h4>

<p>In the “Bag of Words” model a text is represented as a set of its word irregardless of word order or grammar; Only the words themselves and their number of occurence are considered.</p>

<p>Example:</p>

<p><img src="https://pad.gwdg.de/uploads/4d33b422-6c77-436c-a3e6-bf27e67dc203.jpg" alt="A sample paragraph in German Fraktur" /></p>

<blockquote>
  <p>Eine Mondfinsternis ist die Himmelsbegebenheit welche sich zur Zeit des Vollmondes ereignet, wenn die Erde zwischen der Sonne und dem Monde steht, so daß die Strahlen der Sonne von der Erde aufgehalten werden, und daß man so den Schatten der Erde in dem Monde siehet. In diesem Jahre sind zwey Monfinsternisse, davon ist ebenfalls nur Eine bey uns sichtbar, und zwar am 30sten März des Morgens nach 4 Uhr, und währt bis nach 6 Uhr.</p>
</blockquote>

<p>To get the Bag of Words of this paragraph a set containing each word and its number of occurence is created:</p>

<p>$BoW$ =</p>

<pre><code class="language-json=">{
    "Eine": 2, "Mondfinsternis": 1, "ist": 2, "die": 2, "Himmelsbegebenheit": 1, 
    "welche": 1, "sich": 1, "zur": 1,  "Zeit": 1, "des": 2, "Vollmondes": 1,
    "ereignet,": 1, "wenn":1, "Erde": 3, "zwischen": 1, "der": 4, "Sonne": 2,
    "und": 4, "dem": 2, "Monde": 2, "steht,": 1, "so": 2, "daß": 2, 
    "Strahlen": 1, "von": 1, "aufgehalten": 1, "werden,": 1, "man": 1, "den": 1, 
    "Schatten": 1, "in": 1, "siehet.": 1, "In": 1, "diesem": 1, "Jahre": 1, 
    "sind": 1, "zwey": 1, "Monfinsternisse,": 1, "davon": 1, "ebenfalls": 1, "nur": 1, 
    "bey": 1, "uns": 1, "sichtbar,": 1, "zwar": 1, "am": 1, "30sten": 1, 
    "März": 1, "Morgens": 1, "nach": 2, "4": 1, "Uhr,": 1, "währt": 1, 
    "bis": 1, "6": 1, "Uhr.": 1
}
</code></pre>

<h3 id="layout-evaluation">Layout Evaluation</h3>

<p>For documents with a complex structure, looking at the recognized text’s accuracy alone is often insufficient to accurately determine the quality of OCR. An example can help to illustrate this: in a document containing two columns, all characters and words may be recognized correctly, but when the two columns are detected by layout analysis as just one, the OCR result will contain the text for the first lines of the first and second column, followed by the second lines of the first and second column asf., rendering the sequence of words and paragraphs in the Ground Truth text wrongly, which defeats almost all downstream processes.</p>

<p>While the comprehensive evaluation of OCR with consideration of layout analysis is still a research topic, several established metrics can be used to capture different aspects of it.</p>

<h4 id="reading-order">Reading Order</h4>

<p>Reading order describes the order in which segments on a page are intended to be read. While the reading order might be easily obtained in monographs with a single column where only a few page segments exist, identifying the reading order in more complex layouts (e.g. newspapers or multi-column layouts) can be more challenging.</p>

<p>Example of a simple page layout with reading order:</p>

<p><img src="https://pad.gwdg.de/uploads/bc5258cb-bf91-479e-8a91-abf5ff8bbbfa.jpg" alt="A sample page in German Fraktur with a simple page layout showing the intended reading order" />
(<a href="http://resolver.sub.uni-goettingen.de/purl?PPN1726778096">http://resolver.sub.uni-goettingen.de/purl?PPN1726778096</a>)</p>

<p>Example of a complex page layout with reading order:</p>

<p><img src="https://pad.gwdg.de/uploads/100f14c4-19b0-4810-b3e5-74c674575424.jpg" alt="A sample page in German Fraktur with a complex page layout showing the intended reading order" />
(<a href="http://resolver.sub.uni-goettingen.de/purl?PPN1726778096">http://resolver.sub.uni-goettingen.de/purl?PPN1726778096</a>)</p>

<h4 id="iou-intersection-over-union">IoU (Intersection over Union)</h4>

<p>Intersection over Union is a term which describes the degree of overlap of two regions of a (document) image defined either by a bounding box or polygon. Example:</p>

<p><img src="https://pad.gwdg.de/uploads/62945a01-a7a7-48f3-86c2-6bb8f97d67fe.jpg" alt="A sample heading in German Fraktur illustrating a Ground Truth bounding box and a detected bounding box" /></p>

<p>(where green represents the Ground Truth and red the detected bounding box)</p>

<p>Given a region A with an area $area_1$, a region B with the area $area_2$, and their overlap (or intersection) $area_o$, the IoU can then be expressed as</p>

<p>$IoU = \frac{area_o}{area_1+area_2-area_o}$</p>

<p>where $area_1+area_2-area_o$ expresses the union of the two regions ($area_1+area_2$) while not counting the overlapping area twice.</p>

<p>The IoU ranges between 0 (no overlap at all) and 1 (the two regions overlap perfectly). Users executing object detection can choose a <a href="#Threshold">threshold</a> that defines which degree of overlap must be given to define a prediction as correct. If e.g. a threshold of 0.6 is chosen, all prediction that have an IoU of 0.6 or higher are correct.</p>

<p>In OCR-D we use IoU to measure how well segments on a page are recognized during the segmentation step. The area of one region represents the area identified in the Ground Truth, while the second region represents the area identified by an OCR-D processor.</p>

<h3 id="resource-utilization">Resource Utilization</h3>

<p>Last but not least, it is important to collect information about the resource utilization of each processing step, so that informed decisions can be made when e.g. having to decide between results quality and throughput speed.</p>

<h4 id="cpu-time">CPU Time</h4>

<p>CPU time is the time taken by the CPU to process an instruction. It does not include idle time.</p>

<h4 id="wall-time">Wall Time</h4>

<p>Wall time (or elapsed time) is the time taken by a processor to process an instruction including idle time.</p>

<h4 id="io">I/O</h4>

<p>I/O (input / output) is the number of bytes read and written during a process.</p>

<h4 id="memory-usage">Memory Usage</h4>

<p>Memory usage is the number of bytes the process allocates in memory (RAM).</p>

<h4 id="disk-usage">Disk Usage</h4>

<p>Disk usage is the number of bytes the process allocates on hard disk.</p>

<h3 id="unicode-normalization">Unicode normalization</h3>

<p>In Unicode there can be multiple ways to express characters that have multiple components, such as a base letter and an accent. For evaluation it is essential that both Ground Truth and OCR results are normalized <em>in the same way</em> before evaluation.</p>

<p>For example, the letter <code class="language-plaintext highlighter-rouge">ä</code> can be expressed directly as <code class="language-plaintext highlighter-rouge">ä</code> (<code class="language-plaintext highlighter-rouge">U+00E4</code> in Unicode) or as a combination of <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">◌̈</code> (<code class="language-plaintext highlighter-rouge">U+0061 + U+0308</code>). Both encodings are semantically equivalent but technically different.</p>

<p>Unicode has the notion of <em>normalization forms</em> to provide canonically normalized text. The most common forms are <em>NFC</em> (Normalization Form Canonical Composed) and <em>NFD</em> (Normalization Form Canonical Decomposed). When a Unicode string is in NFC, all decomposed codepoints are replaced with their decomposed equivalent (e.g. <code class="language-plaintext highlighter-rouge">U+0061 + U+0308</code> to <code class="language-plaintext highlighter-rouge">U+00E4</code>). In an NFD encoding, all decomposed codepoints are replaced with their composed equivalents (e.g. <code class="language-plaintext highlighter-rouge">U+00E4</code> to <code class="language-plaintext highlighter-rouge">U+0061 + U+0308</code>).</p>

<!-- There's also NFKC and NFKD - necessary to explain? -->

<p>In accordance with the concept of <a href="https://ocr-d.de/en/gt-guidelines/trans/trLevels.html">GT levels in OCR-D</a>, it is preferable for strings to be normalized as NFC.</p>

<p>The Unicode normalization algorithms rely on data from the Unicode database on equivalence classes and other script- and language-related metadata. For graphemes from the Private Use Area (PUA), such as MUFI, this information is not readily available and can lead to inconsistent normalization. Therefore, it is essential that evaluation tools normalize PUA codepoints in addition to canonical Unicode normalization.</p>

<!-- Reference to GT rules here? -->

<h3 id="metrics-not-in-use-yet">Metrics Not in Use Yet</h3>

<p>The following metrics are not part of the MVP (minimal viable product) and will (if ever) be implemented at a later stage.</p>

<h4 id="gpu-metrics">GPU metrics</h4>

<h5 id="gpu-time">GPU time</h5>

<p>GPU time is the time a GPU (graphics card) spent processing instructions</p>

<h5 id="gpu-avg-memory">GPU avg memory</h5>

<p>GPU avg memory refers to the average amount of memory of the GPU (in GiB) that was used during processing.</p>

<h4 id="text-evaluation-1">Text Evaluation</h4>

<h5 id="flexible-character-accuracy-measure">Flexible Character Accuracy Measure</h5>

<p>The flexible character accuracy measure has been introduced to mitigate a major flaw of the CER: The CER is heavily dependent on the reading order an OCR engine detects; When content blocks are e.g. mixed up or merged during the text recognition step but single characters have been perfectly recognized, the CER is still very low.</p>

<p>The flexible character accuracy measure circumvents this effect by splitting the recognized text and the Ground Truth in smaller chunks and measure their partial edit distance. After all partial edit distances have been obtained, they are summed up to receive the overall character accuracy measure.</p>

<p>The algorithm can be summarized as follows:</p>

<blockquote>
  <ol>
    <li>Split the two input texts into text lines</li>
    <li>Sort the ground truth text lines by length (in descending order)</li>
    <li>For the first ground truth line, find the best matching OCR result line segment (by minimising a penalty that is partly based on string edit distance)</li>
    <li>If full match (full length of line)
a. Mark as done and remove line from list
b. Else subdivide and add to respective list of text lines; resort</li>
    <li>If any more lines available repeat step 3</li>
    <li>Count non-matched lines / strings as insertions or deletions (depending on origin: ground truth or result)</li>
    <li>Sum up all partial edit distances and calculate overall character accuracy</li>
  </ol>
</blockquote>

<p>(C. Clausner, S. Pletschacher and A. Antonacopoulos / Pattern Recognition Letters 131 (2020) 390–397, p. 392)</p>

<h4 id="layout-evalutation">Layout Evalutation</h4>

<h5 id="map-mean-average-precision">mAP (mean Average Precision)</h5>

<h6 id="precision-and-recall">Precision and Recall</h6>

<p><strong>Precision</strong> is a means to describe how accurate a model can identify an object within an image. The higher the precision of a model, the more confidently we can assume that a prediction (e.g. the model having identified a bicycle in an image) is correct. A precision of 1 indicates that each identified object in an image has been correctly identified (true positives) and no false positives have been detected. As the precision value descreases, the result contains more and more false positives.</p>

<p><strong>Recall</strong>, on the other hand, measures how well a model performs in finding all instances of an object in an image (true positives), irregardless of false positives. Given a model tries to identify bicycles in an image, a recall of 1 indicates that all bicycles have been found by the model (while not considering other objects that have been falsely labelled as a bicycle).</p>

<h6 id="prediction-score">Prediction Score</h6>

<p>When a model tries to identify objects in an image, it predicts that a certain area in an image represents said object with a certain confidence or prediction score. The prediction score varies between 0 and 1 and represents the percentage of certainty of having correctly identified an object. Given a model tries to identify ornaments on a page. If the model returns an area of a page with a prediction score of 0.6, the model is “60% sure” that this area is an ornament. If this area is then considered to be a positive, depends on the chosen threshold.</p>

<h6 id="thresholds">Thresholds</h6>

<p>A threshold is a freely chosen number between 0 and 1. It divides the output of a model into two groups: Outputs that have a prediction score or IoU greater than or equal to the threshold represent an object. Outputs with a prediction score or IoU below the threshold are discarded as not representing the object.</p>

<p>Example:
Given a threshold of 0.6 and a model that tries to detect bicycles in an image. The model returns two areas in an image that might be bicycles, one with a prediction score of 0.4 and one with 0.9. Since the threshold equals 0.6, the first area is tossed and not regarded as bicycle while the second one is kept and counted as recognized.</p>

<h6 id="precision-recall-curve">Precision-Recall-Curve</h6>

<p>Precision and recall are connected to each other since both depend on the true positives detected. A precision-recall-curve is a means to balance these values while maximizing them.</p>

<p>Given a dataset with 100 images in total of which 50 depict a bicycle. Also given a model trying to identify bicycles on images. The model is run 7 times using the given dataset while gradually increasing the threshold from 0.1 to 0.7.</p>

<table>
  <thead>
    <tr>
      <th>run</th>
      <th>threshold</th>
      <th>true positives</th>
      <th>false positives</th>
      <th>false negatives</th>
      <th>precision</th>
      <th>recall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.1</td>
      <td>50</td>
      <td>25</td>
      <td>0</td>
      <td>0.66</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.2</td>
      <td>45</td>
      <td>20</td>
      <td>5</td>
      <td>0.69</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.3</td>
      <td>40</td>
      <td>15</td>
      <td>10</td>
      <td>0.73</td>
      <td>0.8</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.4</td>
      <td>35</td>
      <td>5</td>
      <td>15</td>
      <td>0.88</td>
      <td>0.7</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.5</td>
      <td>30</td>
      <td>3</td>
      <td>20</td>
      <td>0.91</td>
      <td>0.6</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.6</td>
      <td>20</td>
      <td>0</td>
      <td>30</td>
      <td>1</td>
      <td>0.4</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.7</td>
      <td>10</td>
      <td>0</td>
      <td>40</td>
      <td>1</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>

<p>For each threshold a pair of precision and recall can be computed and plotted to a curve:</p>

<p><img src="https://pad.gwdg.de/uploads/2d3c62ff-cab4-4a12-8043-014fe0440459.png" alt="A sample precision/recall curve" /></p>

<p>This graph is called Precision-Recall-Curve.</p>

<h6 id="average-precision">Average Precision</h6>

<p>The average precision (AP) describes how well a model can detect objects in an image for recall values over 0 to 1 by computing the average of all precisions given in the Precision-Recall-Curve. It is equal to the area under the curve.</p>

<p><img src="https://pad.gwdg.de/uploads/799e6a05-e64a-4956-9ede-440ac0463a3f.png" alt="A sample precision/recall curve with highlighted area under curve" /></p>

<p>The Average Precision can be computed with the weighted mean of precision at each confidence threshold:</p>

<p>$AP = \displaystyle\sum_{k=0}^{k=n-1}[r(k) - r(k+1)] * p(k)$</p>

<p>with $n$ being the number of thresholds and $r(k)$/$p(k)$ being the respective recall/precision values for the current confidence threshold $k$.</p>

<p>Example:
Given the example above, we get:</p>

\[\begin{array}{ll}
AP &amp;  = \displaystyle\sum_{k=0}^{k=n-1}[r(k) - r(k+1)] * p(k) \\
&amp; = \displaystyle\sum_{k=0}^{k=6}[r(k) - r(k+1)] * p(k) \\
&amp; = (1-0.9) * 0.66 + (0.9-0.8) * 0.69 + \text{...} + (0.2-0) * 1\\
&amp; = 0.878
\end{array}\]

<h6 id="map-mean-average-precision-1">mAP (mean Average Precision)</h6>

<p>The mean Average Precision is a metric used to measure how accurate an object detector is. <a href="#Thresholds">As stated</a>, a threshold can be chosen freely, so there is some room for errors when picking one single threshold. To mitigate this effect, the mean Average Precision metric has been introduced which considers a set of IoU thresholds to determine the detector’s performance. It is calculated by first computing the Average Precision for each IoU threshold and then finding the average:</p>

<p>$mAP = \displaystyle\frac{1}{N}\sum_{i=1}^{N}AP_i$ with $N$ being the number of thresholds.</p>

<h5 id="scenario-driven-performance-evaluation">Scenario-driven Performance Evaluation</h5>

<p>Scenario-driven performance evaluation as described in <a href="https://primaresearch.org/publications/ICDAR2011_Clausner_PerformanceEvaluation">Clausner et al., 2011</a> is currently the most comprehensive and sophisticated approach to evaluate OCR success with consideration of layout.</p>

<p>The approach is based on the definition of so called evaluation scenarios, which allow the flexible combination of a selection of metrics together with their weights, targeted at a specific use case.</p>

<h2 id="evaluation-json-schema">Evaluation JSON schema</h2>

<!-- normative -->

<p>The results of an evaluation should be expressed in JSON according to
the <a href="https://ocr-d.de/en/spec/ocrd-eval.schema.json"><code class="language-plaintext highlighter-rouge">ocrd-eval.json</code></a>.</p>

<h2 id="tools">Tools</h2>

<p>See <a href="https://ocr-d.de/en/workflows#evaluation">OCR-D workflow guide</a>.</p>

<h2 id="references">References</h2>

<ul>
  <li>CER/WER:
    <ul>
      <li><a href="https://sites.google.com/site/textdigitisation/qualitymeasures">https://sites.google.com/site/textdigitisation/qualitymeasures</a></li>
      <li><a href="https://towardsdatascience.com/evaluating-ocr-output-quality-with-character-error-rate-cer-and-word-error-rate-wer-853175297510#5aec">https://towardsdatascience.com/evaluating-ocr-output-quality-with-character-error-rate-cer-and-word-error-rate-wer-853175297510#5aec</a></li>
    </ul>
  </li>
  <li>IoU:
    <ul>
      <li><a href="https://medium.com/analytics-vidhya/iou-intersection-over-union-705a39e7acef">https://medium.com/analytics-vidhya/iou-intersection-over-union-705a39e7acef</a></li>
    </ul>
  </li>
  <li>mAP:
    <ul>
      <li><a href="https://blog.paperspace.com/mean-average-precision/">https://blog.paperspace.com/mean-average-precision/</a></li>
      <li><a href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173">https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173</a></li>
    </ul>
  </li>
  <li>BoW:
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">https://en.wikipedia.org/wiki/Bag-of-words_model</a></li>
    </ul>
  </li>
  <li>FCA:
    <ul>
      <li><a href="https://www.primaresearch.org/www/assets/papers/PRL_Clausner_FlexibleCharacterAccuracy.pdf">https://www.primaresearch.org/www/assets/papers/PRL_Clausner_FlexibleCharacterAccuracy.pdf</a></li>
    </ul>
  </li>
  <li>More background on evaluation of OCR
    <ul>
      <li><a href="https://doi.org/10.1145/3476887.3476888">https://doi.org/10.1145/3476887.3476888</a></li>
      <li><a href="https://doi.org/10.1515/9783110691597-009">https://doi.org/10.1515/9783110691597-009</a></li>
    </ul>
  </li>
</ul>

      </main>
    </div><footer class="footer" style="padding: 1rem">
    <div class="content has-text-centered">
      <img class="footer-logo" src="/assets/dfg_logo_eng.jpg" alt="DFG logo"/>
    </div>
    <!-- <div class="content has-text-centered"> -->
    <!--   <img class="footer-logo" src="/assets/logo-bbaw.png" alt="BBAW logo"/> -->
    <!--   <img class="footer-logo" src="/assets/logo-hab.gif" alt="HAB logo"/> -->
    <!--   <img class="footer-logo" src="/assets/logo-kit.png" alt="KIT logo"/> -->
    <!--   <img class="footer-logo" src="/assets/logo-sbb.png" alt="SBB logo"/> -->
    <!-- </div> -->
    <div class="content has-text-centered">
		<a href="https://github.com/OCR-D">GitHub</a>
		|
		<a href="https://gitter.im/OCR-D/Lobby">Gitter</a>
		|
		<a href="https://twitter.com/OCR_D_community">Twitter</a>
		|
		<a href="https://github.com/OCR-D/ocrd-website/wiki">Wiki</a>
		|
		<a href="https://hub.docker.com/u/ocrd">Docker Hub</a>
		|
		<a href="https://www.zotero.org/groups/418719/ocr-d">Technology Watch</a>
		|
		<a href="/sitemap.xml">sitemap.xml</a>
		|
		
			<a href="/en/imprint">Imprint</a>
		
    </div>

<script src="/assets/script.js"></script>
</footer>
</body>

</html>
